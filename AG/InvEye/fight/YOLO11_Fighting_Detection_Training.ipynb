{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ‘Š YOLO11n Fighting/Violence Detection Training\n",
                "\n",
                "**Multiple Datasets: Roboflow + Kaggle + GitHub**\n",
                "\n",
                "**Run cells in order!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install packages\n",
                "!pip install -q ultralytics>=8.3.0 roboflow gdown\n",
                "\n",
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nâœ… CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import os\n",
                "import glob\n",
                "import shutil\n",
                "import yaml\n",
                "import torch\n",
                "import zipfile\n",
                "from ultralytics import YOLO\n",
                "from IPython.display import display, Image as IPImage\n",
                "\n",
                "MERGED_PATH = \"/content/violence_merged_dataset\"\n",
                "print(\"âœ… Ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Datasets\n",
                "\n",
                "We'll download from multiple sources:\n",
                "1. **Roboflow** - Your forked dataset\n",
                "2. **Kaggle** - Fight detection YOLO dataset\n",
                "3. **HuggingFace/GitHub** - Public violence datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "# ========================================\n",
                "# ðŸ”‘ PASTE YOUR ROBOFLOW API KEY HERE\n",
                "# ========================================\n",
                "ROBOFLOW_API_KEY = \"YOUR_API_KEY_HERE\"\n",
                "\n",
                "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
                "print(\"âœ… Connected to Roboflow!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 1: Your forked Roboflow dataset (WORKS!)\n",
                "# ========================================\n",
                "try:\n",
                "    project1 = rf.workspace(\"securityviolence\").project(\"violence-detection-p4qev\")\n",
                "    dataset1 = project1.version(1).download(\"yolov8\")\n",
                "    print(f\"âœ… Dataset 1 (Roboflow): {dataset1.location}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 1 failed: {e}\")\n",
                "    dataset1 = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 2: Kaggle - Fight Detection YOLO Dataset\n",
                "# Direct download from public source\n",
                "# ========================================\n",
                "import gdown\n",
                "\n",
                "# Fight Detection Dataset (from HuggingFace - Musawer14/fight_detection_yolov8)\n",
                "# This is a public dataset with YOLO annotations\n",
                "\n",
                "print(\"ðŸ“¥ Downloading Dataset 2 from HuggingFace...\")\n",
                "\n",
                "try:\n",
                "    !wget -q https://huggingface.co/datasets/Musawer14/fight_detection_yolov8/resolve/main/data.zip -O /content/dataset2.zip\n",
                "    \n",
                "    # Extract\n",
                "    with zipfile.ZipFile('/content/dataset2.zip', 'r') as z:\n",
                "        z.extractall('/content/dataset2')\n",
                "    \n",
                "    dataset2_path = '/content/dataset2'\n",
                "    print(f\"âœ… Dataset 2 (HuggingFace): {dataset2_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 2 failed: {e}\")\n",
                "    dataset2_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 3: Real Life Violence Situations (Kaggle)\n",
                "# Extracted frames with violence/non-violence\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 3 (Real Life Violence)...\")\n",
                "\n",
                "try:\n",
                "    # This is a direct download link to violence detection images\n",
                "    !wget -q --no-check-certificate 'https://github.com/vdquang1991/TNUE_FightDetection/archive/refs/heads/main.zip' -O /content/dataset3.zip\n",
                "    \n",
                "    with zipfile.ZipFile('/content/dataset3.zip', 'r') as z:\n",
                "        z.extractall('/content/dataset3')\n",
                "    \n",
                "    dataset3_path = '/content/dataset3/TNUE_FightDetection-main'\n",
                "    print(f\"âœ… Dataset 3 (GitHub): {dataset3_path}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 3 failed: {e}\")\n",
                "    dataset3_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check what we have\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ“Š DOWNLOAD SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "if dataset1:\n",
                "    c1 = len(glob.glob(f\"{dataset1.location}/train/images/*\")) + len(glob.glob(f\"{dataset1.location}/valid/images/*\"))\n",
                "    print(f\"Dataset 1 (Roboflow): âœ… {c1} images\")\n",
                "else:\n",
                "    print(\"Dataset 1 (Roboflow): âŒ\")\n",
                "\n",
                "if dataset2_path and os.path.exists(dataset2_path):\n",
                "    print(f\"Dataset 2 (HuggingFace): âœ… Downloaded\")\n",
                "    !ls {dataset2_path}\n",
                "else:\n",
                "    print(\"Dataset 2 (HuggingFace): âŒ\")\n",
                "\n",
                "if dataset3_path and os.path.exists(dataset3_path):\n",
                "    print(f\"Dataset 3 (GitHub): âœ… Downloaded\")\n",
                "    !ls {dataset3_path}\n",
                "else:\n",
                "    print(\"Dataset 3 (GitHub): âŒ\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare & Merge Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create merged directory\n",
                "for split in ['train', 'valid', 'test']:\n",
                "    os.makedirs(f\"{MERGED_PATH}/images/{split}\", exist_ok=True)\n",
                "    os.makedirs(f\"{MERGED_PATH}/labels/{split}\", exist_ok=True)\n",
                "\n",
                "# Class mapping\n",
                "CLASS_MAP = {\n",
                "    'violence': 0, 'Violence': 0, 'VIOLENCE': 0,\n",
                "    'fight': 0, 'Fight': 0, 'fighting': 0, 'Fighting': 0,\n",
                "    'Violence/Fight': 0, 'violent': 0,\n",
                "    \n",
                "    'non_violence': 1, 'NonViolence': 1, 'nonviolence': 1,\n",
                "    'no_fight': 1, 'NoFight': 1, 'normal': 1, 'Normal': 1,\n",
                "    'NoViolence/NoFight': 1, 'non-violence': 1\n",
                "}\n",
                "\n",
                "CLASSES = {0: 'violence', 1: 'non_violence'}\n",
                "print(f\"âœ… Unified Classes: {CLASSES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_roboflow_dataset(ds, prefix):\n",
                "    \"\"\"Merge a Roboflow dataset\"\"\"\n",
                "    if ds is None:\n",
                "        return 0\n",
                "    \n",
                "    src = ds.location\n",
                "    count = 0\n",
                "    \n",
                "    yaml_path = f\"{src}/data.yaml\"\n",
                "    if not os.path.exists(yaml_path):\n",
                "        return 0\n",
                "    \n",
                "    with open(yaml_path, 'r') as f:\n",
                "        cfg = yaml.safe_load(f)\n",
                "    \n",
                "    src_classes = cfg.get('names', {})\n",
                "    if isinstance(src_classes, list):\n",
                "        src_classes = {i: n for i, n in enumerate(src_classes)}\n",
                "    \n",
                "    print(f\"   Classes: {src_classes}\")\n",
                "    \n",
                "    for split in ['train', 'valid', 'test']:\n",
                "        img_dir = f\"{src}/{split}/images\"\n",
                "        lbl_dir = f\"{src}/{split}/labels\"\n",
                "        \n",
                "        if not os.path.exists(img_dir):\n",
                "            continue\n",
                "        \n",
                "        for img in glob.glob(f\"{img_dir}/*\"):\n",
                "            name = os.path.basename(img)\n",
                "            base, ext = os.path.splitext(name)\n",
                "            \n",
                "            new_name = f\"{prefix}_{base}{ext}\"\n",
                "            shutil.copy(img, f\"{MERGED_PATH}/images/{split}/{new_name}\")\n",
                "            \n",
                "            lbl = f\"{lbl_dir}/{base}.txt\"\n",
                "            if os.path.exists(lbl):\n",
                "                with open(lbl, 'r') as f:\n",
                "                    lines = f.readlines()\n",
                "                \n",
                "                new_lines = []\n",
                "                for line in lines:\n",
                "                    parts = line.strip().split()\n",
                "                    if len(parts) >= 5:\n",
                "                        old_id = int(parts[0])\n",
                "                        old_name = src_classes.get(old_id, '')\n",
                "                        new_id = CLASS_MAP.get(old_name)\n",
                "                        if new_id is not None:\n",
                "                            new_lines.append(f\"{new_id} {' '.join(parts[1:])}\\n\")\n",
                "                \n",
                "                if new_lines:\n",
                "                    with open(f\"{MERGED_PATH}/labels/{split}/{prefix}_{base}.txt\", 'w') as f:\n",
                "                        f.writelines(new_lines)\n",
                "            \n",
                "            count += 1\n",
                "    \n",
                "    return count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_yolo_folder(src_path, prefix):\n",
                "    \"\"\"Merge a YOLO-format dataset from a folder\"\"\"\n",
                "    if src_path is None or not os.path.exists(src_path):\n",
                "        return 0\n",
                "    \n",
                "    count = 0\n",
                "    \n",
                "    # Try to find data.yaml\n",
                "    yaml_files = glob.glob(f\"{src_path}/**/data.yaml\", recursive=True)\n",
                "    if not yaml_files:\n",
                "        yaml_files = glob.glob(f\"{src_path}/**/*.yaml\", recursive=True)\n",
                "    \n",
                "    if yaml_files:\n",
                "        yaml_path = yaml_files[0]\n",
                "        base_dir = os.path.dirname(yaml_path)\n",
                "        \n",
                "        with open(yaml_path, 'r') as f:\n",
                "            cfg = yaml.safe_load(f)\n",
                "        \n",
                "        src_classes = cfg.get('names', {})\n",
                "        if isinstance(src_classes, list):\n",
                "            src_classes = {i: n for i, n in enumerate(src_classes)}\n",
                "        \n",
                "        print(f\"   Found yaml: {yaml_path}\")\n",
                "        print(f\"   Classes: {src_classes}\")\n",
                "        \n",
                "        for split in ['train', 'valid', 'test', 'val']:\n",
                "            target_split = 'valid' if split == 'val' else split\n",
                "            \n",
                "            # Try different folder structures\n",
                "            possible_paths = [\n",
                "                f\"{base_dir}/{split}/images\",\n",
                "                f\"{base_dir}/images/{split}\",\n",
                "                f\"{src_path}/{split}/images\",\n",
                "                f\"{src_path}/images/{split}\"\n",
                "            ]\n",
                "            \n",
                "            img_dir = None\n",
                "            for p in possible_paths:\n",
                "                if os.path.exists(p):\n",
                "                    img_dir = p\n",
                "                    break\n",
                "            \n",
                "            if not img_dir:\n",
                "                continue\n",
                "            \n",
                "            lbl_dir = img_dir.replace('images', 'labels')\n",
                "            \n",
                "            for img in glob.glob(f\"{img_dir}/*\"):\n",
                "                name = os.path.basename(img)\n",
                "                base, ext = os.path.splitext(name)\n",
                "                \n",
                "                if ext.lower() not in ['.jpg', '.jpeg', '.png', '.bmp', '.webp']:\n",
                "                    continue\n",
                "                \n",
                "                new_name = f\"{prefix}_{base}{ext}\"\n",
                "                shutil.copy(img, f\"{MERGED_PATH}/images/{target_split}/{new_name}\")\n",
                "                \n",
                "                lbl = f\"{lbl_dir}/{base}.txt\"\n",
                "                if os.path.exists(lbl):\n",
                "                    with open(lbl, 'r') as f:\n",
                "                        lines = f.readlines()\n",
                "                    \n",
                "                    new_lines = []\n",
                "                    for line in lines:\n",
                "                        parts = line.strip().split()\n",
                "                        if len(parts) >= 5:\n",
                "                            old_id = int(parts[0])\n",
                "                            old_name = src_classes.get(old_id, '')\n",
                "                            new_id = CLASS_MAP.get(old_name)\n",
                "                            if new_id is not None:\n",
                "                                new_lines.append(f\"{new_id} {' '.join(parts[1:])}\\n\")\n",
                "                    \n",
                "                    if new_lines:\n",
                "                        with open(f\"{MERGED_PATH}/labels/{target_split}/{prefix}_{base}.txt\", 'w') as f:\n",
                "                            f.writelines(new_lines)\n",
                "                \n",
                "                count += 1\n",
                "    \n",
                "    return count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge all datasets\n",
                "print(\"ðŸ”„ Merging datasets...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "total = 0\n",
                "\n",
                "# Dataset 1 - Roboflow\n",
                "print(\"\\nðŸ“¦ Processing Dataset 1 (Roboflow)...\")\n",
                "n1 = merge_roboflow_dataset(dataset1, \"rf\")\n",
                "print(f\"   âœ… Merged {n1} images\")\n",
                "total += n1\n",
                "\n",
                "# Dataset 2 - HuggingFace\n",
                "if dataset2_path:\n",
                "    print(\"\\nðŸ“¦ Processing Dataset 2 (HuggingFace)...\")\n",
                "    n2 = merge_yolo_folder(dataset2_path, \"hf\")\n",
                "    print(f\"   âœ… Merged {n2} images\")\n",
                "    total += n2\n",
                "\n",
                "# Dataset 3 - GitHub\n",
                "if dataset3_path:\n",
                "    print(\"\\nðŸ“¦ Processing Dataset 3 (GitHub)...\")\n",
                "    n3 = merge_yolo_folder(dataset3_path, \"gh\")\n",
                "    print(f\"   âœ… Merged {n3} images\")\n",
                "    total += n3\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"âœ… TOTAL MERGED: {total} images\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data.yaml\n",
                "data_yaml_path = f\"{MERGED_PATH}/data.yaml\"\n",
                "\n",
                "with open(data_yaml_path, 'w') as f:\n",
                "    yaml.dump({\n",
                "        'path': MERGED_PATH,\n",
                "        'train': 'images/train',\n",
                "        'val': 'images/valid',\n",
                "        'test': 'images/test',\n",
                "        'names': CLASSES\n",
                "    }, f)\n",
                "\n",
                "# Final stats\n",
                "train = len(glob.glob(f\"{MERGED_PATH}/images/train/*\"))\n",
                "valid = len(glob.glob(f\"{MERGED_PATH}/images/valid/*\"))\n",
                "\n",
                "print(f\"âœ… Created: {data_yaml_path}\")\n",
                "print(f\"\\nðŸ“Š Final Dataset:\")\n",
                "print(f\"   Train: {train}\")\n",
                "print(f\"   Valid: {valid}\")\n",
                "print(f\"   Total: {train + valid}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train Model (YOLO11n)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "MODEL = 'yolo11n.pt'  # nano - fastest for Jetson\n",
                "EPOCHS = 100\n",
                "BATCH = 16\n",
                "IMG_SIZE = 640\n",
                "\n",
                "PROJECT = 'violence_detection'\n",
                "NAME = 'yolo11n_violence'\n",
                "\n",
                "print(f\"âœ… Model: {MODEL}, Epochs: {EPOCHS}, Batch: {BATCH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "model = YOLO(MODEL)\n",
                "print(f\"âœ… Loaded {MODEL}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train\n",
                "print(\"ðŸš€ Starting training...\")\n",
                "\n",
                "results = model.train(\n",
                "    data=data_yaml_path,\n",
                "    epochs=EPOCHS,\n",
                "    batch=BATCH,\n",
                "    imgsz=IMG_SIZE,\n",
                "    patience=30,\n",
                "    project=PROJECT,\n",
                "    name=NAME,\n",
                "    exist_ok=True,\n",
                "    device=0,\n",
                "    cache=True,\n",
                "    amp=True,\n",
                "    plots=True\n",
                ")\n",
                "\n",
                "print(\"\\nâœ… Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RESULTS_DIR = f\"{PROJECT}/{NAME}\"\n",
                "\n",
                "if os.path.exists(f\"{RESULTS_DIR}/results.png\"):\n",
                "    display(IPImage(filename=f\"{RESULTS_DIR}/results.png\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validate\n",
                "best_model = YOLO(f\"{RESULTS_DIR}/weights/best.pt\")\n",
                "val = best_model.val(data=data_yaml_path)\n",
                "\n",
                "print(f\"\\nmAP50: {val.box.map50:.4f}\")\n",
                "print(f\"mAP50-95: {val.box.map:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Export & Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export ONNX\n",
                "onnx = best_model.export(format='onnx', imgsz=IMG_SIZE, simplify=True)\n",
                "print(f\"âœ… ONNX: {onnx}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download\n",
                "from google.colab import files\n",
                "\n",
                "weights = f\"{RESULTS_DIR}/weights\"\n",
                "files.download(f\"{weights}/best.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download ONNX\n",
                "files.download(f\"{weights}/best.onnx\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸŽ‰ Done!\n",
                "\n",
                "### Deploy on Jetson:\n",
                "```bash\n",
                "yolo export model=best.pt format=engine device=0 half=True\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}