
LIVE VIDEO ANALYTICS FOR PETROL PUMPS - COMPLETE IMPLEMENTATION GUIDE
=====================================================================

KEY INSIGHT: Why Batch Processing is Not Sustainable
====================================================
You mentioned analyzing entire videos in Maigic was expensive. Here's why:

BATCH PROCESSING COSTS:
- Entire video must be uploaded to cloud (2-8 MB/min)
- Full GPU processing time charged (often $0.30-0.50/minute)
- For 24-hour continuous monitoring: $432-720 per camera per month
- Doesn't scale: 10 cameras = $4,320-7,200/month

LIVE PROCESSING SOLUTION:
- Only send numerical data (1-10 KB instead of full video)
- Process locally on edge first, send insights to Maigic
- Cost reduction: 99.5% bandwidth + 80-90% compute
- 10 cameras = $150-300/month (hybrid approach)

================================================================================
PART 1: STREAM INGESTION & FRAME SAMPLING (The Cost Killer Step)
================================================================================

The #1 cost factor is bandwidth. You need aggressive frame sampling.

APPROACH 1: Threading-based RTSP Stream (Recommended for Real-Time)
-------------------------------------------------------------------

import cv2
import threading
import queue
import time
from collections import deque

class RTSPStreamBuffer:
    def __init__(self, rtsp_url, buffer_size=2, target_fps=3):
        self.rtsp_url = rtsp_url
        self.buffer_size = buffer_size
        self.target_fps = target_fps
        self.frame_queue = queue.Queue(maxsize=buffer_size)
        self.running = False

    def read_frames(self):
        """Continuously read frames in background thread"""
        cap = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)

        # Disable buffering to reduce latency
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)

        frame_interval = int(1000 / self.target_fps)  # Convert to milliseconds
        last_frame_time = time.time()
        frame_count = 0
        skipped_frames = 0

        while self.running and cap.isOpened():
            ret, frame = cap.read()

            if not ret:
                time.sleep(0.1)
                continue

            # FRAME SAMPLING LOGIC - Skip frames based on FPS target
            current_time = time.time()
            elapsed_ms = (current_time - last_frame_time) * 1000

            if elapsed_ms >= frame_interval:
                try:
                    # Non-blocking put to avoid blocking on full queue
                    self.frame_queue.put_nowait(frame)
                    last_frame_time = current_time
                    frame_count += 1
                except queue.Full:
                    skipped_frames += 1
                    # Queue full means we're processing too slow
                    # Metric: high skip rate = need edge GPU
            else:
                skipped_frames += 1

        cap.release()
        print(f"Stream ended. Processed: {frame_count}, Skipped: {skipped_frames}")

    def start(self):
        self.running = True
        thread = threading.Thread(target=self.read_frames, daemon=True)
        thread.start()
        return thread

    def get_frame(self, timeout=1):
        try:
            return self.frame_queue.get(timeout=timeout)
        except queue.Empty:
            return None

    def stop(self):
        self.running = False

# USAGE EXAMPLE:
stream = RTSPStreamBuffer(
    rtsp_url="rtsp://camera_ip:554/stream",
    buffer_size=2,        # Small buffer = low latency
    target_fps=3          # 3 FPS = 60% bandwidth reduction vs 30fps
)

thread = stream.start()

frame_count = 0
while frame_count < 100:
    frame = stream.get_frame(timeout=2)
    if frame is None:
        print("No frame available - network issue or too slow processing")
        continue

    # Process frame here (detection, etc.)
    frame_count += 1

    if frame_count % 30 == 0:
        print(f"Processed {frame_count} frames")

stream.stop()


================================================================================
PART 2: FRAME PREPROCESSING - BANDWIDTH REDUCTION
================================================================================

Reducing resolution from 1080p to 720p = 60% bandwidth reduction
Reducing from 1080p to 480p = 80% bandwidth reduction
Accuracy impact on YOLO = minimal (still >90% accurate)

def preprocess_frame(frame, target_resolution=(640, 480)):
    """
    Preprocess frame for minimal bandwidth and computation

    Standard YOLO input: 640x640 or 416x416
    Petrol pump: 640x480 works well (wide aspect ratio)
    """
    import numpy as np

    # Resize
    height, width = frame.shape[:2]
    target_h, target_w = target_resolution
    frame_resized = cv2.resize(frame, (target_w, target_h))

    # Normalize to [0, 1] for deep learning
    frame_normalized = frame_resized.astype(np.float32) / 255.0

    # Optional: Convert BGR to RGB (many models expect RGB)
    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)
    frame_rgb_normalized = frame_rgb.astype(np.float32) / 255.0

    return frame_rgb_normalized

# DATA VOLUME COMPARISON:
# Original 1080p frame: 1920 × 1080 × 3 × 1 byte = 6.2 MB
# After resize to 640×480: 640 × 480 × 3 = 921.6 KB (85% reduction!)
# At 3 FPS: 921.6 KB × 3 = 2.76 MB/sec = 165 MB/minute


================================================================================
PART 3: YOLO OBJECT DETECTION ON EDGE
================================================================================

Critical: Run this on edge GPU, NOT cloud. This is where most cost is saved.

from ultralytics import YOLO
import numpy as np

class PetrolPumpDetector:
    def __init__(self, model_size='nano'):  # nano < small < medium < large
        # yolov8n = 3.2 MB, fastest (nanoscale)
        # yolov8s = 11.2 MB, balanced
        # Use 'nano' for edge, 'small' for better accuracy
        self.model = YOLO(f'yolov8{model_size[0]}.pt')
        self.model.to('cuda')  # Use GPU if available

    def detect_objects(self, frame, conf_threshold=0.5):
        """
        Detect vehicles and people in frame
        Returns: list of detections with coordinates and confidence
        """
        results = self.model(frame, conf=conf_threshold, verbose=False)

        detections = []
        for result in results:
            for box in result.boxes:
                detection = {
                    'class_id': int(box.cls[0].cpu().numpy()),
                    'class_name': result.names[int(box.cls[0])],
                    'confidence': float(box.conf[0].cpu().numpy()),
                    'bbox': {
                        'x1': float(box.xyxy[0][0].cpu().numpy()),
                        'y1': float(box.xyxy[0][1].cpu().numpy()),
                        'x2': float(box.xyxy[0][2].cpu().numpy()),
                        'y2': float(box.xyxy[0][3].cpu().numpy())
                    }
                }
                detections.append(detection)

        return detections, results[0].im  # Return original annotated image

# PERFORMANCE METRICS:
# YOLOv8n (nano):  ~65 FPS on RTX 3060
# YOLOv8s (small): ~45 FPS on RTX 3060
# Processing cost: ~0.015 ms per frame on GPU (negligible)
# Edge hardware cost: $150-300/month (one-time amortized)


================================================================================
PART 4: OBJECT TRACKING - LIGHTWEIGHT APPROACH
================================================================================

Track vehicles and people across frames with minimal overhead

from collections import defaultdict
import math

class SimpleObjectTracker:
    def __init__(self, max_distance=50):
        self.tracks = {}  # {track_id: [(x, y, timestamp), ...]}
        self.next_track_id = 1
        self.max_distance = max_distance
        self.frame_count = 0

    def calculate_distance(self, box1, box2):
        """Calculate centroid distance between two bboxes"""
        c1_x = (box1['x1'] + box1['x2']) / 2
        c1_y = (box1['y1'] + box1['y2']) / 2

        c2_x = (box2['x1'] + box2['x2']) / 2
        c2_y = (box2['y1'] + box2['y2']) / 2

        distance = math.sqrt((c1_x - c2_x)**2 + (c1_y - c2_y)**2)
        return distance

    def update(self, detections, timestamp):
        """Update tracks with new detections"""
        self.frame_count += 1

        # Assign detections to existing tracks
        matched_detections = set()
        updated_tracks = {}

        for track_id, track in self.tracks.items():
            # Get last known position
            last_x = (track[-1]['bbox']['x1'] + track[-1]['bbox']['x2']) / 2
            last_y = (track[-1]['bbox']['y1'] + track[-1]['bbox']['y2']) / 2

            # Find closest detection
            best_detection_idx = None
            best_distance = self.max_distance

            for i, det in enumerate(detections):
                if i in matched_detections:
                    continue

                distance = self.calculate_distance(
                    track[-1]['bbox'], 
                    det['bbox']
                )

                if distance < best_distance:
                    best_distance = distance
                    best_detection_idx = i

            # Update track if match found
            if best_detection_idx is not None:
                new_track = track + [{
                    'detection': detections[best_detection_idx],
                    'timestamp': timestamp,
                    'bbox': detections[best_detection_idx]['bbox']
                }]
                # Keep last 10 frames (30 frames = 10 seconds @ 3 FPS)
                updated_tracks[track_id] = new_track[-10:]
                matched_detections.add(best_detection_idx)
            else:
                # Track without new detection (will be removed after timeout)
                if len(track) < 5:  # Remove if not seen for 5 frames
                    pass
                else:
                    updated_tracks[track_id] = track

        # Create new tracks for unmatched detections
        for i, det in enumerate(detections):
            if i not in matched_detections:
                new_track_id = self.next_track_id
                self.next_track_id += 1
                updated_tracks[new_track_id] = [{
                    'detection': det,
                    'timestamp': timestamp,
                    'bbox': det['bbox']
                }]

        self.tracks = updated_tracks
        return updated_tracks


================================================================================
PART 5: PETROL PUMP SPECIFIC KPI EXTRACTION
================================================================================

Convert detections to actionable business metrics

import datetime
import json

class PetrolPumpAnalytics:
    def __init__(self):
        self.vehicle_entries = []
        self.vehicle_exits = []
        self.current_vehicles_in_area = 0
        self.current_people_count = 0
        self.pump_occupancy = {}  # {pump_id: [timestamps]}
        self.service_times = []

    def extract_kpis(self, tracks, detections, frame_shape):
        """
        Extract KPIs specific to petrol pump operations
        """
        kpis = {
            'timestamp': datetime.datetime.now().isoformat(),
            'frame_shape': frame_shape,
            'current_detections': {
                'vehicles': 0,
                'people': 0,
                'total_objects': len(detections)
            },
            'tracking': {
                'active_tracks': len(tracks),
                'track_ids': list(tracks.keys())
            },
            'queue_metrics': {},
            'operational_metrics': {}
        }

        # Count object types
        vehicle_count = sum(1 for d in detections if d['class_id'] in [2, 5, 7])  # car, truck, bus
        person_count = sum(1 for d in detections if d['class_id'] == 0)  # person

        kpis['current_detections']['vehicles'] = vehicle_count
        kpis['current_detections']['people'] = person_count

        # Queue analysis (simplified: count vehicles)
        # In production: use zone detection to identify queue vs service areas
        queue_length = vehicle_count
        kpis['queue_metrics']['current_queue_length'] = queue_length
        kpis['queue_metrics']['estimated_wait_time'] = queue_length * 5  # 5 min per vehicle

        # Pump utilization (simplified)
        kpis['operational_metrics']['pumps_in_use'] = max(1, vehicle_count)
        kpis['operational_metrics']['utilization_rate'] = min(1.0, vehicle_count / 3.0)  # 3 pumps

        return kpis

    def generate_analytics_report(self, kpis_list):
        """
        Generate aggregated report from KPI history
        """
        if not kpis_list:
            return None

        report = {
            'period': {
                'start': kpis_list[0]['timestamp'],
                'end': kpis_list[-1]['timestamp'],
                'num_samples': len(kpis_list)
            },
            'aggregated_metrics': {
                'avg_queue_length': sum(k['queue_metrics']['current_queue_length'] 
                                       for k in kpis_list) / len(kpis_list),
                'max_queue_length': max(k['queue_metrics']['current_queue_length'] 
                                       for k in kpis_list),
                'avg_people_count': sum(k['current_detections']['people'] 
                                       for k in kpis_list) / len(kpis_list),
                'total_vehicle_detections': sum(k['current_detections']['vehicles'] 
                                               for k in kpis_list),
            }
        }

        return report


================================================================================
PART 6: COMPLETE EDGE PROCESSING PIPELINE
================================================================================

Main loop combining all components - this runs on edge device

def run_edge_analytics_pipeline(rtsp_url, output_queue):
    """
    Complete pipeline running on edge device (Jetson, RTX GPU, etc.)

    Args:
        rtsp_url: Camera stream URL
        output_queue: Queue to send processed data to cloud (Maigic)
    """
    import time
    import threading

    # Initialize components
    stream = RTSPStreamBuffer(rtsp_url, buffer_size=2, target_fps=3)
    detector = PetrolPumpDetector(model_size='nano')
    tracker = SimpleObjectTracker(max_distance=50)
    analytics = PetrolPumpAnalytics()

    # Start streaming thread
    stream.start()

    frame_count = 0
    kpi_buffer = []  # Aggregate KPIs before sending

    print("Edge analytics pipeline started. Processing at 3 FPS...")

    while True:
        try:
            # Get frame from buffer
            frame = stream.get_frame(timeout=2)
            if frame is None:
                continue

            # Preprocess
            frame_processed = preprocess_frame(frame)

            # Detect objects
            detections, annotated_frame = detector.detect_objects(frame)

            # Track objects
            timestamp = time.time()
            tracks = tracker.update(detections, timestamp)

            # Extract KPIs
            kpis = analytics.extract_kpis(tracks, detections, frame.shape)
            kpi_buffer.append(kpis)

            frame_count += 1

            # Send aggregated data to cloud every 10 seconds (or ~30 frames @ 3 FPS)
            if frame_count % 30 == 0:
                report = analytics.generate_analytics_report(kpi_buffer)

                # THIS is what gets sent to Maigic (NOT the video!)
                # Just ~2-5 KB of JSON data instead of video
                output_data = {
                    'camera_id': 'camera_001',
                    'timestamp': datetime.datetime.now().isoformat(),
                    'kpis': report,
                    'latest_kpi': kpis
                }

                try:
                    output_queue.put_nowait(output_data)
                    print(f"[Frame {frame_count}] Sent aggregated KPIs to cloud")
                    kpi_buffer = []  # Reset for next batch
                except:
                    print(f"[Frame {frame_count}] Output queue full - skipping send")

        except Exception as e:
            print(f"Error in pipeline: {e}")
            time.sleep(0.1)

=======================================================================================================
