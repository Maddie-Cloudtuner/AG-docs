{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üõ¢Ô∏è Indian Petrol Pump Analytics - YOLO11 Training\n",
                "\n",
                "**All-in-One Notebook**: Downloads public datasets, merges with class remapping, and trains YOLO11.\n",
                "\n",
                "| Class ID | Name | Class ID | Name |\n",
                "|----------|------|----------|------|\n",
                "| 0 | person | 9 | testing_jar |\n",
                "| 1 | car | 10 | du_cover_open |\n",
                "| 2 | motorcycle | 11 | manhole_open |\n",
                "| 3 | heavy_vehicle | 12 | air_pump |\n",
                "| 4 | fire | 13 | uniform |\n",
                "| 5 | smoke | 14 | helmet |\n",
                "| 6 | cigarette | 15 | plastic_item |\n",
                "| 7 | violence | 16 | garbage |\n",
                "| 8 | nozzle | 17 | cell_phone |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 1Ô∏è‚É£ Setup & Install Dependencies\n",
                "!pip install -q ultralytics gdown\n",
                "import os, shutil, random, yaml, gdown, zipfile\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "from google.colab import drive\n",
                "\n",
                "# Mount Google Drive for saving model\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "BASE_DIR = Path('/content')\n",
                "DATASETS_DIR = BASE_DIR / 'source_datasets'\n",
                "OUTPUT_DIR = BASE_DIR / 'Final_Dataset'\n",
                "DATASETS_DIR.mkdir(exist_ok=True)\n",
                "print('‚úÖ Setup complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 2Ô∏è‚É£ Master Schema & Mappings\n",
                "\n",
                "MASTER_SCHEMA = {\n",
                "    0: 'person', 1: 'car', 2: 'motorcycle', 3: 'heavy_vehicle',\n",
                "    4: 'fire', 5: 'smoke', 6: 'cigarette', 7: 'violence',\n",
                "    8: 'nozzle', 9: 'testing_jar', 10: 'du_cover_open', 11: 'manhole_open',\n",
                "    12: 'air_pump', 13: 'uniform', 14: 'helmet', 15: 'plastic_item',\n",
                "    16: 'garbage', 17: 'cell_phone'\n",
                "}\n",
                "\n",
                "# Source dataset mappings: {source_class_id: target_class_id}\n",
                "DATASET_CONFIGS = {\n",
                "    'fire_smoke': {\n",
                "        'url': 'https://github.com/spacewalk01/fire-smoke-detection-yolov8/releases/download/v1.0/fire-smoke-dataset.zip',\n",
                "        'mapping': {0: 4, 1: 5},  # fire->4, smoke->5\n",
                "    },\n",
                "    'violence': {\n",
                "        'url': 'https://drive.google.com/uc?id=1_2LQj-FhKdXzXxjv_giQ4qv_kA4Lcnqm',\n",
                "        'mapping': {0: 7},  # violence->7\n",
                "    },\n",
                "    'ppe': {\n",
                "        'url': 'https://drive.google.com/uc?id=1MGbLfEY_rXvO61dEedG7vK7k7K0aM9ND',\n",
                "        'mapping': {0: 14, 1: 13, 2: 0},  # hardhat->14, vest->13, person->0\n",
                "    },\n",
                "    'cigarette': {\n",
                "        'url': 'https://drive.google.com/uc?id=1kZNd78UPtfMfKXSYtLl9wjCHX7lcdQFT',\n",
                "        'mapping': {0: 6},  # cigarette->6\n",
                "    },\n",
                "    'phone': {\n",
                "        'url': 'https://drive.google.com/uc?id=1yU9PgFRQRlsD9XA2djBo6xvP8l8P3KU4',\n",
                "        'mapping': {0: 17},  # cell_phone->17\n",
                "    },\n",
                "}\n",
                "\n",
                "print(f'‚úÖ Configured {len(DATASET_CONFIGS)} datasets')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 3Ô∏è‚É£ Download Datasets (Public URLs - No API Key)\n",
                "\n",
                "def download_and_extract(name, url, dest_dir):\n",
                "    \"\"\"Download and extract a dataset.\"\"\"\n",
                "    dest_dir = Path(dest_dir)\n",
                "    zip_path = dest_dir / f'{name}.zip'\n",
                "    extract_dir = dest_dir / name\n",
                "    \n",
                "    if extract_dir.exists():\n",
                "        print(f'  ‚è≠Ô∏è {name} already exists, skipping...')\n",
                "        return extract_dir\n",
                "    \n",
                "    print(f'  üì• Downloading {name}...')\n",
                "    try:\n",
                "        if 'drive.google.com' in url:\n",
                "            gdown.download(url, str(zip_path), quiet=True)\n",
                "        else:\n",
                "            !wget -q -O \"{zip_path}\" \"{url}\"\n",
                "        \n",
                "        # Extract\n",
                "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
                "            z.extractall(extract_dir)\n",
                "        zip_path.unlink()  # Remove zip\n",
                "        print(f'  ‚úÖ {name} extracted')\n",
                "        return extract_dir\n",
                "    except Exception as e:\n",
                "        print(f'  ‚ùå Failed: {e}')\n",
                "        return None\n",
                "\n",
                "print('üîÑ Downloading datasets...')\n",
                "for name, config in DATASET_CONFIGS.items():\n",
                "    download_and_extract(name, config['url'], DATASETS_DIR)\n",
                "print('\\n‚úÖ All downloads complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 4Ô∏è‚É£ Merge Datasets with Class Remapping\n",
                "\n",
                "def setup_output_dirs(output_dir):\n",
                "    \"\"\"Create output directory structure.\"\"\"\n",
                "    output_dir = Path(output_dir)\n",
                "    for split in ['train', 'val']:\n",
                "        (output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
                "        (output_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
                "    return output_dir\n",
                "\n",
                "def remap_labels(label_path, mapping):\n",
                "    \"\"\"Remap class IDs in a label file.\"\"\"\n",
                "    if not label_path.exists():\n",
                "        return []\n",
                "    lines = []\n",
                "    with open(label_path) as f:\n",
                "        for line in f:\n",
                "            parts = line.strip().split()\n",
                "            if len(parts) >= 5:\n",
                "                src_id = int(parts[0])\n",
                "                if src_id in mapping and mapping[src_id] is not None:\n",
                "                    lines.append(f\"{mapping[src_id]} {' '.join(parts[1:])}\")\n",
                "    return lines\n",
                "\n",
                "def find_yolo_structure(base_path):\n",
                "    \"\"\"Find images/labels folders in various structures.\"\"\"\n",
                "    base_path = Path(base_path)\n",
                "    candidates = [\n",
                "        (base_path / 'images', base_path / 'labels'),\n",
                "        (base_path / 'train' / 'images', base_path / 'train' / 'labels'),\n",
                "        (base_path / 'data' / 'images', base_path / 'data' / 'labels'),\n",
                "    ]\n",
                "    # Also check subdirectories\n",
                "    for subdir in base_path.iterdir():\n",
                "        if subdir.is_dir():\n",
                "            candidates.append((subdir / 'images', subdir / 'labels'))\n",
                "    \n",
                "    for img_dir, lbl_dir in candidates:\n",
                "        if img_dir.exists():\n",
                "            return img_dir, lbl_dir\n",
                "    return None, None\n",
                "\n",
                "def merge_dataset(name, source_dir, mapping, output_dir, stats):\n",
                "    \"\"\"Merge a single dataset into output.\"\"\"\n",
                "    source_dir = Path(source_dir)\n",
                "    output_dir = Path(output_dir)\n",
                "    \n",
                "    img_dir, lbl_dir = find_yolo_structure(source_dir)\n",
                "    if img_dir is None:\n",
                "        print(f'  ‚ö†Ô∏è Could not find YOLO structure in {source_dir}')\n",
                "        return 0\n",
                "    \n",
                "    count = 0\n",
                "    img_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}\n",
                "    \n",
                "    for split in ['train', 'val', '']:\n",
                "        img_split_dir = img_dir / split if split else img_dir\n",
                "        lbl_split_dir = lbl_dir / split if split else lbl_dir\n",
                "        out_split = 'train' if split in ['train', ''] else 'val'\n",
                "        \n",
                "        if not img_split_dir.exists():\n",
                "            continue\n",
                "            \n",
                "        for img_path in img_split_dir.glob('*'):\n",
                "            if img_path.suffix.lower() not in img_exts:\n",
                "                continue\n",
                "            \n",
                "            lbl_path = lbl_split_dir / f'{img_path.stem}.txt'\n",
                "            remapped = remap_labels(lbl_path, mapping)\n",
                "            \n",
                "            if not remapped:\n",
                "                continue\n",
                "            \n",
                "            # Copy with unique name\n",
                "            unique_name = f'{name}_{img_path.stem}'\n",
                "            shutil.copy2(img_path, output_dir / 'images' / out_split / f'{unique_name}{img_path.suffix}')\n",
                "            with open(output_dir / 'labels' / out_split / f'{unique_name}.txt', 'w') as f:\n",
                "                f.write('\\n'.join(remapped))\n",
                "            \n",
                "            count += 1\n",
                "            for line in remapped:\n",
                "                cls_id = int(line.split()[0])\n",
                "                stats[cls_id] += 1\n",
                "    \n",
                "    return count\n",
                "\n",
                "# Execute merge\n",
                "print('üîÑ Merging datasets with class remapping...')\n",
                "output_dir = setup_output_dirs(OUTPUT_DIR)\n",
                "stats = defaultdict(int)\n",
                "total = 0\n",
                "\n",
                "for name, config in DATASET_CONFIGS.items():\n",
                "    source_dir = DATASETS_DIR / name\n",
                "    if source_dir.exists():\n",
                "        count = merge_dataset(name, source_dir, config['mapping'], output_dir, stats)\n",
                "        print(f'  ‚úÖ {name}: {count} images merged')\n",
                "        total += count\n",
                "\n",
                "print(f'\\nüìä Total: {total} images merged')\n",
                "print('\\nüìà Class Distribution:')\n",
                "for cls_id in sorted(MASTER_SCHEMA.keys()):\n",
                "    count = stats.get(cls_id, 0)\n",
                "    marker = '‚úÖ' if count > 0 else '‚ö†Ô∏è'\n",
                "    print(f'  {cls_id:2d}: {MASTER_SCHEMA[cls_id]:<15} = {count:>5} {marker}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 5Ô∏è‚É£ Generate data.yaml\n",
                "\n",
                "yaml_content = {\n",
                "    'path': str(OUTPUT_DIR),\n",
                "    'train': 'images/train',\n",
                "    'val': 'images/val',\n",
                "    'nc': len(MASTER_SCHEMA),\n",
                "    'names': MASTER_SCHEMA\n",
                "}\n",
                "\n",
                "yaml_path = OUTPUT_DIR / 'data.yaml'\n",
                "with open(yaml_path, 'w') as f:\n",
                "    yaml.dump(yaml_content, f, default_flow_style=False, sort_keys=False)\n",
                "\n",
                "print(f'‚úÖ Generated: {yaml_path}')\n",
                "print('\\nüìÑ data.yaml contents:')\n",
                "!cat {yaml_path}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 6Ô∏è‚É£ Train YOLO11 Model\n",
                "from ultralytics import YOLO\n",
                "\n",
                "# Load YOLO11 nano model (optimized for Jetson)\n",
                "model = YOLO('yolo11n.pt')\n",
                "\n",
                "# Training configuration\n",
                "results = model.train(\n",
                "    data=str(yaml_path),\n",
                "    epochs=100,\n",
                "    imgsz=640,\n",
                "    batch=16,\n",
                "    patience=20,\n",
                "    device=0,  # GPU\n",
                "    workers=4,\n",
                "    project='/content/runs',\n",
                "    name='petrol_pump_yolo11',\n",
                "    exist_ok=True,\n",
                "    amp=True,  # Mixed precision\n",
                "    augment=True,\n",
                "    hsv_h=0.015,\n",
                "    hsv_s=0.7,\n",
                "    hsv_v=0.4,\n",
                "    degrees=10,\n",
                "    translate=0.1,\n",
                "    scale=0.5,\n",
                "    flipud=0.0,\n",
                "    fliplr=0.5,\n",
                "    mosaic=1.0,\n",
                "    mixup=0.1,\n",
                ")\n",
                "\n",
                "print('‚úÖ Training complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 7Ô∏è‚É£ Export to ONNX (for Jetson Orin Nano)\n",
                "from ultralytics import YOLO\n",
                "\n",
                "best_model = YOLO('/content/runs/petrol_pump_yolo11/weights/best.pt')\n",
                "\n",
                "# Export to ONNX\n",
                "best_model.export(format='onnx', imgsz=640, simplify=True, opset=12)\n",
                "\n",
                "print('‚úÖ ONNX export complete!')\n",
                "print('üìÅ Model files:')\n",
                "!ls -la /content/runs/petrol_pump_yolo11/weights/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 8Ô∏è‚É£ Save to Google Drive\n",
                "import shutil\n",
                "\n",
                "drive_path = '/content/drive/MyDrive/PetrolPump_YOLO11'\n",
                "os.makedirs(drive_path, exist_ok=True)\n",
                "\n",
                "# Copy model files\n",
                "shutil.copy('/content/runs/petrol_pump_yolo11/weights/best.pt', f'{drive_path}/petrol_pump_yolo11_best.pt')\n",
                "shutil.copy('/content/runs/petrol_pump_yolo11/weights/best.onnx', f'{drive_path}/petrol_pump_yolo11_best.onnx')\n",
                "shutil.copy(str(yaml_path), f'{drive_path}/data.yaml')\n",
                "\n",
                "print(f'‚úÖ Saved to: {drive_path}')\n",
                "!ls -la \"{drive_path}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title 9Ô∏è‚É£ Test Inference (Optional)\n",
                "from ultralytics import YOLO\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "model = YOLO('/content/runs/petrol_pump_yolo11/weights/best.pt')\n",
                "\n",
                "# Run inference on validation images\n",
                "val_images = list((OUTPUT_DIR / 'images' / 'val').glob('*.jpg'))[:5]\n",
                "\n",
                "if val_images:\n",
                "    results = model.predict(source=val_images, save=True, conf=0.25)\n",
                "    print(f'‚úÖ Inference complete on {len(val_images)} images')\n",
                "    print('üìÅ Results saved to: runs/detect/predict/')\n",
                "else:\n",
                "    print('‚ö†Ô∏è No validation images found')"
            ]
        }
    ]
}