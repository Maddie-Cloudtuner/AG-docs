{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ‘¤ YOLO11n Face Detection & Recognition Training\n",
                "\n",
                "**Petrol Pump Deployment - Employee & Customer Face Analytics**\n",
                "\n",
                "**Multiple Public Datasets: Roboflow + Kaggle + WIDER FACE**\n",
                "\n",
                "**Run cells in order!**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“‹ Use Cases for Petrol Pump\n",
                "\n",
                "| Use Case | Description |\n",
                "|----------|-------------|\n",
                "| **Employee Attendance** | Auto clock-in/out via face recognition |\n",
                "| **Unknown Person Alert** | Detect unfamiliar faces in restricted areas |\n",
                "| **Customer Analytics** | Count unique customers, demographics |\n",
                "| **VIP Detection** | Alert when regular/VIP customers arrive |\n",
                "| **Theft Prevention** | Flag known offenders |\n",
                "| **Safety Compliance** | Verify employee identity at pumps |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & GPU Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install packages\n",
                "!pip install -q ultralytics>=8.3.0 roboflow gdown opencv-python-headless kaggle\n",
                "\n",
                "# Check GPU\n",
                "!nvidia-smi\n",
                "\n",
                "import torch\n",
                "print(f\"\\nâœ… CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"âœ… Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core Imports\n",
                "import os\n",
                "import glob\n",
                "import shutil\n",
                "import yaml\n",
                "import torch\n",
                "import zipfile\n",
                "import random\n",
                "import cv2\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from ultralytics import YOLO\n",
                "from IPython.display import display, Image as IPImage, clear_output\n",
                "\n",
                "# Paths\n",
                "MERGED_PATH = \"/content/face_detection_dataset\"\n",
                "RESULTS_DIR = \"/content/face_detection_results\"\n",
                "\n",
                "print(\"âœ… All imports ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Download Public Face Datasets\n",
                "\n",
                "We'll download from multiple verified public sources:\n",
                "1. **Roboflow WIDER Face** - Large-scale benchmark (WORKS âœ…)\n",
                "2. **Kaggle Face Detection** - 16.7k images with YOLO annotations\n",
                "3. **FDDB Roboflow** - Classic benchmark converted to YOLO\n",
                "4. **YOLO Face Detection** - Verified Roboflow dataset\n",
                "5. **WIDER Face Original** - HuggingFace (WORKS âœ…)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from roboflow import Roboflow\n",
                "\n",
                "# ========================================\n",
                "# ðŸ”‘ PASTE YOUR ROBOFLOW API KEY HERE\n",
                "# Get it from: https://app.roboflow.com/settings/api\n",
                "# ========================================\n",
                "ROBOFLOW_API_KEY = \"YOUR_API_KEY_HERE\"\n",
                "\n",
                "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
                "print(\"âœ… Connected to Roboflow!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 1: WIDER Face Detection (Roboflow) - VERIFIED WORKING\n",
                "# Large-scale dataset with 32k+ images\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 1: WIDER Face (Roboflow)...\")\n",
                "\n",
                "try:\n",
                "    project1 = rf.workspace(\"mohamed-traore-2ekkp\").project(\"face-detection-mik1i\")\n",
                "    dataset1 = project1.version(18).download(\"yolov8\")\n",
                "    print(f\"âœ… Dataset 1 (WIDER Face): {dataset1.location}\")\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 1 failed: {e}\")\n",
                "    dataset1 = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 2: Kaggle Face Detection (16.7k images) - NEW\n",
                "# Direct download with YOLO annotations\n",
                "# Source: https://kaggle.com/datasets/fareselmenshawii/face-detection-dataset\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 2: Kaggle Face Detection...\")\n",
                "print(\"\\nâš ï¸ Kaggle requires authentication!\")\n",
                "print(\"   1. Go to https://www.kaggle.com/settings\")\n",
                "print(\"   2. Click 'Create New Token' to download kaggle.json\")\n",
                "print(\"   3. Upload kaggle.json when prompted below\\n\")\n",
                "\n",
                "from google.colab import files\n",
                "\n",
                "# Check if kaggle.json already exists\n",
                "os.makedirs('/root/.kaggle', exist_ok=True)\n",
                "kaggle_json_path = '/root/.kaggle/kaggle.json'\n",
                "\n",
                "try:\n",
                "    if not os.path.exists(kaggle_json_path):\n",
                "        print(\"ðŸ“¤ Please upload your kaggle.json file:\")\n",
                "        uploaded = files.upload()\n",
                "        \n",
                "        for filename in uploaded.keys():\n",
                "            if filename == 'kaggle.json':\n",
                "                with open(kaggle_json_path, 'wb') as f:\n",
                "                    f.write(uploaded[filename])\n",
                "                os.chmod(kaggle_json_path, 0o600)\n",
                "                print(\"âœ… Kaggle credentials saved!\")\n",
                "    \n",
                "    # Download dataset\n",
                "    !kaggle datasets download -d fareselmenshawii/face-detection-dataset -p /content/dataset2_kaggle --unzip -q\n",
                "    \n",
                "    dataset2_path = '/content/dataset2_kaggle'\n",
                "    if os.path.exists(dataset2_path) and len(os.listdir(dataset2_path)) > 0:\n",
                "        print(f\"âœ… Dataset 2 (Kaggle): {dataset2_path}\")\n",
                "        !ls {dataset2_path}\n",
                "    else:\n",
                "        dataset2_path = None\n",
                "        print(\"âš ï¸ Dataset 2 directory empty\")\n",
                "        \n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 2 (Kaggle) failed: {e}\")\n",
                "    print(\"   You can skip this and continue with other datasets\")\n",
                "    dataset2_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 3: FDDB-style Face Detection (Roboflow) - NEW\n",
                "# Various public face detection datasets\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 3: Roboflow Face Datasets...\")\n",
                "\n",
                "dataset3 = None\n",
                "\n",
                "# Try multiple Roboflow sources\n",
                "roboflow_sources = [\n",
                "    (\"human-faces-object-detection\", \"faces-t8ou2\", 1, \"Human Faces\"),\n",
                "    (\"selfdriving-car-4ithn\", \"face-detection-t4v2d\", 2, \"Car Face Detection\"),\n",
                "    (\"facial-emotion\", \"yolo-face-detection\", 1, \"YOLO Face\"),\n",
                "    (\"yolo-project-0jqzr\", \"face-detection-urcfj\", 1, \"YOLO Project\"),\n",
                "]\n",
                "\n",
                "for workspace, project_name, version, desc in roboflow_sources:\n",
                "    try:\n",
                "        print(f\"   Trying: {desc}...\")\n",
                "        proj = rf.workspace(workspace).project(project_name)\n",
                "        dataset3 = proj.version(version).download(\"yolov8\")\n",
                "        print(f\"âœ… Dataset 3 ({desc}): {dataset3.location}\")\n",
                "        break\n",
                "    except Exception as e:\n",
                "        print(f\"   âš ï¸ {desc} failed: {str(e)[:50]}...\")\n",
                "        continue\n",
                "\n",
                "if dataset3 is None:\n",
                "    print(\"âš ï¸ All Dataset 3 sources failed - continuing without it\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 4: Additional Face Detection (Roboflow) - NEW\n",
                "# More face detection datasets\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 4: Additional Face Detection...\")\n",
                "\n",
                "dataset4 = None\n",
                "\n",
                "# Try additional sources\n",
                "additional_sources = [\n",
                "    (\"face-detection-yolo8n\", \"face-detection-u2umf\", 1, \"Face Detection YOLOv8n\"),\n",
                "    (\"face-detection-mnspc\", \"face-detection-wpjtl\", 1, \"Face Detection 2\"),\n",
                "    (\"face-detection-ehwbh\", \"face-detection-kpsbl\", 1, \"Face Detection 3\"),\n",
                "]\n",
                "\n",
                "for workspace, project_name, version, desc in additional_sources:\n",
                "    try:\n",
                "        print(f\"   Trying: {desc}...\")\n",
                "        proj = rf.workspace(workspace).project(project_name)\n",
                "        dataset4 = proj.version(version).download(\"yolov8\")\n",
                "        print(f\"âœ… Dataset 4 ({desc}): {dataset4.location}\")\n",
                "        break\n",
                "    except Exception as e:\n",
                "        print(f\"   âš ï¸ {desc} failed: {str(e)[:50]}...\")\n",
                "        continue\n",
                "\n",
                "if dataset4 is None:\n",
                "    print(\"âš ï¸ All Dataset 4 sources failed - continuing without it\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ========================================\n",
                "# ðŸ“Š DATASET 5: WIDER Face from HuggingFace - VERIFIED WORKING\n",
                "# Download WIDER Face benchmark dataset\n",
                "# ========================================\n",
                "print(\"ðŸ“¥ Downloading Dataset 5: WIDER Face (HuggingFace)...\")\n",
                "\n",
                "try:\n",
                "    # Download WIDER Face training set\n",
                "    !wget -q --show-progress https://huggingface.co/datasets/wider_face/resolve/main/data/WIDER_train.zip -O /content/wider_train.zip\n",
                "    \n",
                "    # Extract\n",
                "    with zipfile.ZipFile('/content/wider_train.zip', 'r') as z:\n",
                "        z.extractall('/content/dataset5')\n",
                "    \n",
                "    dataset5_path = '/content/dataset5'\n",
                "    print(f\"âœ… Dataset 5 (WIDER Original): {dataset5_path}\")\n",
                "    !ls {dataset5_path}\n",
                "except Exception as e:\n",
                "    print(f\"âš ï¸ Dataset 5 failed: {e}\")\n",
                "    dataset5_path = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check what we have downloaded\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ“Š DOWNLOAD SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "total_downloaded = 0\n",
                "\n",
                "# Check Roboflow datasets\n",
                "for i, (ds, name) in enumerate([\n",
                "    (dataset1, \"WIDER Face (Roboflow)\"),\n",
                "    (dataset3, \"Roboflow Face Detection\"),\n",
                "    (dataset4, \"Additional Face Detection\")\n",
                "], 1):\n",
                "    if ds is not None:\n",
                "        try:\n",
                "            train_imgs = len(glob.glob(f\"{ds.location}/train/images/*\"))\n",
                "            valid_imgs = len(glob.glob(f\"{ds.location}/valid/images/*\"))\n",
                "            total = train_imgs + valid_imgs\n",
                "            print(f\"Dataset {i} ({name}): âœ… {total} images\")\n",
                "            total_downloaded += total\n",
                "        except:\n",
                "            print(f\"Dataset {i} ({name}): âš ï¸ Could not count\")\n",
                "    else:\n",
                "        print(f\"Dataset {i} ({name}): âŒ Not downloaded\")\n",
                "\n",
                "# Check Kaggle dataset\n",
                "if dataset2_path and os.path.exists(dataset2_path):\n",
                "    kaggle_imgs = len(glob.glob(f\"{dataset2_path}/**/*.jpg\", recursive=True))\n",
                "    kaggle_imgs += len(glob.glob(f\"{dataset2_path}/**/*.png\", recursive=True))\n",
                "    print(f\"Dataset 2 (Kaggle): âœ… {kaggle_imgs} images\")\n",
                "    total_downloaded += kaggle_imgs\n",
                "else:\n",
                "    print(f\"Dataset 2 (Kaggle): âŒ Not downloaded\")\n",
                "\n",
                "# Check WIDER Face HuggingFace\n",
                "if dataset5_path and os.path.exists(dataset5_path):\n",
                "    print(f\"Dataset 5 (WIDER HuggingFace): âœ… Downloaded\")\n",
                "else:\n",
                "    print(f\"Dataset 5 (WIDER HuggingFace): âŒ Not downloaded\")\n",
                "\n",
                "print(f\"\\nðŸ“Š Total images downloaded: ~{total_downloaded:,}+\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare & Merge Datasets\n",
                "\n",
                "Unify all datasets into a single YOLO-compatible format with consistent class IDs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create merged directory structure\n",
                "for split in ['train', 'valid', 'test']:\n",
                "    os.makedirs(f\"{MERGED_PATH}/images/{split}\", exist_ok=True)\n",
                "    os.makedirs(f\"{MERGED_PATH}/labels/{split}\", exist_ok=True)\n",
                "\n",
                "# Unified class mapping - all face variants map to class 0\n",
                "CLASS_MAP = {\n",
                "    'face': 0, 'Face': 0, 'FACE': 0,\n",
                "    'human_face': 0, 'human-face': 0,\n",
                "    'person_face': 0, 'head': 0,\n",
                "    0: 0, '0': 0,\n",
                "}\n",
                "\n",
                "# Single class output\n",
                "CLASSES = {0: 'face'}\n",
                "print(f\"âœ… Unified Classes: {CLASSES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def merge_roboflow_dataset(ds, prefix):\n",
                "    \"\"\"Merge a Roboflow-downloaded dataset into the unified format.\"\"\"\n",
                "    if ds is None:\n",
                "        return 0\n",
                "    \n",
                "    src = ds.location\n",
                "    count = 0\n",
                "    \n",
                "    yaml_path = f\"{src}/data.yaml\"\n",
                "    if not os.path.exists(yaml_path):\n",
                "        print(f\"   âš ï¸ No data.yaml found at {yaml_path}\")\n",
                "        return 0\n",
                "    \n",
                "    for split in ['train', 'valid', 'test']:\n",
                "        img_dir = f\"{src}/{split}/images\"\n",
                "        lbl_dir = f\"{src}/{split}/labels\"\n",
                "        \n",
                "        if not os.path.exists(img_dir):\n",
                "            continue\n",
                "        \n",
                "        for img_path in glob.glob(f\"{img_dir}/*\"):\n",
                "            name = os.path.basename(img_path)\n",
                "            base, ext = os.path.splitext(name)\n",
                "            \n",
                "            if ext.lower() not in ['.jpg', '.jpeg', '.png', '.bmp', '.webp']:\n",
                "                continue\n",
                "            \n",
                "            new_name = f\"{prefix}_{base}{ext}\"\n",
                "            shutil.copy(img_path, f\"{MERGED_PATH}/images/{split}/{new_name}\")\n",
                "            \n",
                "            lbl_path = f\"{lbl_dir}/{base}.txt\"\n",
                "            if os.path.exists(lbl_path):\n",
                "                with open(lbl_path, 'r') as f:\n",
                "                    lines = f.readlines()\n",
                "                \n",
                "                new_lines = []\n",
                "                for line in lines:\n",
                "                    parts = line.strip().split()\n",
                "                    if len(parts) >= 5:\n",
                "                        # Map all classes to 0 (face)\n",
                "                        new_lines.append(f\"0 {' '.join(parts[1:])}\\n\")\n",
                "                \n",
                "                if new_lines:\n",
                "                    with open(f\"{MERGED_PATH}/labels/{split}/{prefix}_{base}.txt\", 'w') as f:\n",
                "                        f.writelines(new_lines)\n",
                "            \n",
                "            count += 1\n",
                "    \n",
                "    return count\n",
                "\n",
                "def merge_kaggle_dataset(dataset_path, prefix):\n",
                "    \"\"\"Merge Kaggle face detection dataset.\"\"\"\n",
                "    if dataset_path is None or not os.path.exists(dataset_path):\n",
                "        return 0\n",
                "    \n",
                "    count = 0\n",
                "    \n",
                "    # Find images and labels directories\n",
                "    for split in ['train', 'val', 'valid', 'test']:\n",
                "        target_split = 'valid' if split == 'val' else split\n",
                "        \n",
                "        # Try different directory structures\n",
                "        possible_img_dirs = [\n",
                "            f\"{dataset_path}/{split}/images\",\n",
                "            f\"{dataset_path}/images/{split}\",\n",
                "            f\"{dataset_path}/{split}\",\n",
                "        ]\n",
                "        \n",
                "        for img_dir in possible_img_dirs:\n",
                "            if os.path.exists(img_dir):\n",
                "                lbl_dir = img_dir.replace('images', 'labels')\n",
                "                \n",
                "                for img_path in glob.glob(f\"{img_dir}/*\"):\n",
                "                    name = os.path.basename(img_path)\n",
                "                    base, ext = os.path.splitext(name)\n",
                "                    \n",
                "                    if ext.lower() not in ['.jpg', '.jpeg', '.png']:\n",
                "                        continue\n",
                "                    \n",
                "                    new_name = f\"{prefix}_{base}{ext}\"\n",
                "                    \n",
                "                    # Default to train if no valid split structure\n",
                "                    dest_split = target_split if split != dataset_path else 'train'\n",
                "                    \n",
                "                    shutil.copy(img_path, f\"{MERGED_PATH}/images/{dest_split}/{new_name}\")\n",
                "                    \n",
                "                    lbl_path = f\"{lbl_dir}/{base}.txt\"\n",
                "                    if os.path.exists(lbl_path):\n",
                "                        with open(lbl_path, 'r') as f:\n",
                "                            lines = f.readlines()\n",
                "                        \n",
                "                        new_lines = []\n",
                "                        for line in lines:\n",
                "                            parts = line.strip().split()\n",
                "                            if len(parts) >= 5:\n",
                "                                new_lines.append(f\"0 {' '.join(parts[1:])}\\n\")\n",
                "                        \n",
                "                        if new_lines:\n",
                "                            with open(f\"{MERGED_PATH}/labels/{dest_split}/{prefix}_{base}.txt\", 'w') as f:\n",
                "                                f.writelines(new_lines)\n",
                "                    \n",
                "                    count += 1\n",
                "                break\n",
                "    \n",
                "    return count\n",
                "\n",
                "print(\"âœ… Merge functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Merge all datasets\n",
                "print(\"ðŸ”„ Merging datasets...\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "total_images = 0\n",
                "\n",
                "# Dataset 1 - WIDER Face (Roboflow)\n",
                "print(\"\\nðŸ“¦ Processing Dataset 1 (WIDER Face)...\")\n",
                "n1 = merge_roboflow_dataset(dataset1, \"wider\")\n",
                "print(f\"   âœ… Merged {n1} images\")\n",
                "total_images += n1\n",
                "\n",
                "# Dataset 2 - Kaggle\n",
                "if dataset2_path:\n",
                "    print(\"\\nðŸ“¦ Processing Dataset 2 (Kaggle)...\")\n",
                "    n2 = merge_kaggle_dataset(dataset2_path, \"kaggle\")\n",
                "    print(f\"   âœ… Merged {n2} images\")\n",
                "    total_images += n2\n",
                "\n",
                "# Dataset 3 - Additional Roboflow\n",
                "print(\"\\nðŸ“¦ Processing Dataset 3 (Roboflow)...\")\n",
                "n3 = merge_roboflow_dataset(dataset3, \"rf3\")\n",
                "print(f\"   âœ… Merged {n3} images\")\n",
                "total_images += n3\n",
                "\n",
                "# Dataset 4 - Additional Roboflow\n",
                "print(\"\\nðŸ“¦ Processing Dataset 4 (Roboflow)...\")\n",
                "n4 = merge_roboflow_dataset(dataset4, \"rf4\")\n",
                "print(f\"   âœ… Merged {n4} images\")\n",
                "total_images += n4\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"âœ… TOTAL IMAGES MERGED: {total_images}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create unified data.yaml\n",
                "data_yaml_path = f\"{MERGED_PATH}/data.yaml\"\n",
                "\n",
                "yaml_content = {\n",
                "    'path': MERGED_PATH,\n",
                "    'train': 'images/train',\n",
                "    'val': 'images/valid',\n",
                "    'test': 'images/test',\n",
                "    'nc': 1,\n",
                "    'names': {0: 'face'}\n",
                "}\n",
                "\n",
                "with open(data_yaml_path, 'w') as f:\n",
                "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
                "\n",
                "# Final stats\n",
                "train_count = len(glob.glob(f\"{MERGED_PATH}/images/train/*\"))\n",
                "valid_count = len(glob.glob(f\"{MERGED_PATH}/images/valid/*\"))\n",
                "test_count = len(glob.glob(f\"{MERGED_PATH}/images/test/*\"))\n",
                "\n",
                "print(f\"âœ… Created: {data_yaml_path}\")\n",
                "print(f\"\\nðŸ“Š FINAL DATASET STATISTICS:\")\n",
                "print(f\"   Training:   {train_count:,} images\")\n",
                "print(f\"   Validation: {valid_count:,} images\")\n",
                "print(f\"   Testing:    {test_count:,} images\")\n",
                "print(f\"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
                "print(f\"   TOTAL:      {train_count + valid_count + test_count:,} images\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train YOLO11n Face Detection Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "MODEL = 'yolo11n.pt'\n",
                "EPOCHS = 100\n",
                "BATCH_SIZE = 16\n",
                "IMG_SIZE = 640\n",
                "PATIENCE = 30\n",
                "\n",
                "PROJECT_NAME = 'face_detection'\n",
                "RUN_NAME = 'yolo11n_petrol_pump'\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ðŸ”§ TRAINING CONFIGURATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Model:      {MODEL}\")\n",
                "print(f\"Epochs:     {EPOCHS}\")\n",
                "print(f\"Batch Size: {BATCH_SIZE}\")\n",
                "print(f\"Image Size: {IMG_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and train\n",
                "model = YOLO(MODEL)\n",
                "print(f\"âœ… Loaded base model: {MODEL}\")\n",
                "\n",
                "print(\"\\nðŸš€ STARTING TRAINING...\")\n",
                "results = model.train(\n",
                "    data=data_yaml_path,\n",
                "    epochs=EPOCHS,\n",
                "    batch=BATCH_SIZE,\n",
                "    imgsz=IMG_SIZE,\n",
                "    patience=PATIENCE,\n",
                "    project=PROJECT_NAME,\n",
                "    name=RUN_NAME,\n",
                "    exist_ok=True,\n",
                "    device=0,\n",
                "    cache=True,\n",
                "    amp=True,\n",
                "    plots=True,\n",
                ")\n",
                "\n",
                "print(\"\\nâœ… TRAINING COMPLETE!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluate & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RESULTS_PATH = f\"{PROJECT_NAME}/{RUN_NAME}\"\n",
                "\n",
                "# Show results\n",
                "if os.path.exists(f\"{RESULTS_PATH}/results.png\"):\n",
                "    display(IPImage(filename=f\"{RESULTS_PATH}/results.png\", width=900))\n",
                "\n",
                "# Validate\n",
                "best_model = YOLO(f\"{RESULTS_PATH}/weights/best.pt\")\n",
                "val = best_model.val(data=data_yaml_path)\n",
                "\n",
                "print(\"\\nðŸ“Š VALIDATION RESULTS\")\n",
                "print(f\"mAP50:     {val.box.map50:.4f}\")\n",
                "print(f\"mAP50-95:  {val.box.map:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to ONNX\n",
                "onnx_path = best_model.export(format='onnx', imgsz=IMG_SIZE, simplify=True)\n",
                "print(f\"âœ… ONNX exported: {onnx_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download models\n",
                "from google.colab import files\n",
                "\n",
                "print(\"ðŸ“¥ Downloading best.pt...\")\n",
                "files.download(f\"{RESULTS_PATH}/weights/best.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download ONNX\n",
                "print(\"ðŸ“¥ Downloading ONNX model...\")\n",
                "files.download(onnx_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## ðŸŽ‰ Done!\n",
                "\n",
                "### Deploy on Jetson:\n",
                "```bash\n",
                "yolo export model=best.pt format=engine device=0 half=True\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}